{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the Word document\n",
    "# doc = Document(\"resume.docx\")\n",
    "\n",
    "# # Extract all text\n",
    "# full_text = []\n",
    "# for para in doc.paragraphs:\n",
    "#     full_text.append(para.text)\n",
    "\n",
    "# # Join and print\n",
    "# text = \"\\n\".join(full_text)\n",
    "# # print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'resume.docx'}, page_content='YIJIN BAO\\n\\n\\t\\t+1-773-490-7200\\n\\n\\t\\tChicago, Illinois\\n\\n\\t\\tkay062@uchicago.edu, LinkedIn | GitHub\\n\\n\\n\\n\\n\\neducation\\n\\n\\tHarris School of Public Policy, University of Chicago\\tChicago, IL\\n\\n\\tData Analytics Specialization of Master of Public Policy (MPP), Graduate with Honor\\tJune 2024\\n\\nData Science: ETL pipeline, NLP, Deep Learning, Machine Learning, Computer Vision\\n\\nData Analytics: data collection, management, cleaning, manipulation, sanity check, analysis, visualization, GIS\\n\\nPublic Policy analysis: modeling and research for transportation policy, healthcare policy, urban policy\\n\\nBusiness data product: dashboard design, interactive chatbot, database management, automated workflows\\n\\n\\n\\n\\tShanghai International Studies University\\tShanghai, China\\n\\n\\tDual Bachelor of Laws and English, Outstanding Undergraduate Thesis Honor\\tJuly 2022\\n\\nInternational Law; International Economics Law; International Finance Law\\n\\nHonored Student Scholarship in 2018-2019 Academic Year\\n\\nStudent Union President (2020-2021): maintained alumni network and organize career networking events\\n\\n\\n\\nexperience\\n\\n\\tUnited nations statisticS DIVISION\\tHybrid\\n\\n\\tStatistical Intern using Python\\t February 2025 – June 2025\\n\\nDeveloped a web-scraping program using Amazon Bedrock, called Claude Sonnet 3 model and engineered prompt to optimize AI’s performance of the accurate collection of unstructured event data from international organizations’ websites\\n\\nAutomated and the scraping and processing of user interaction data of Global Network of Data Officers and Statisticians, continuously refining the process and analyzing participation trends to enhance outreach and network management\\n\\n\\n\\nIllinois Criminal Justice information authority\\t                                                                               Chicago, IL\\n\\n\\tState Government Agency | Research Analysis Intern using R, SQL\\tOct 2024 – Mar 2025, June 2023 – Aug 2023\\n\\nEngineered a highly automated data pipeline to efficiently extract from PDF, convert to R-readable format and integrate 50 years of federal disposition data, pre-processed data for analysis and conduct sanity check with complete documentation  \\n\\nCleaned, validated, and transformed a 20GB+ Illinois Traffic Stop dataset (2004–2022) to ensure data quality, standardize formats, and generate performance metrics, enabling statistical analysis of equity in police pull-overs, including visualizing hit rates and false alarm rates by race and police agency as part of the Veil of Darkness analysis\\n\\nAudited data of Illinois Death in Custody Reporting from 2019 to 2024 by developing a program to automate matching and identify unreported incidents through cross-referencing manually collected media-reported cases \\n\\n\\n\\n\\tRural Advancement Foundation International (RAFI)\\tChicago, IL\\n\\n\\tNGO for Economic Justice | Data science fellow at Data Science Clinic using Python\\t Jan 2024 – Mar 2024\\n\\nDeveloped and maintained a Streamlit dashboard to visualize 30 years of market trends across 3,455 auction houses, supporting RAFI’s efforts to convey findings to a Federal Government Agency for digitalization of contracts\\n\\nImproved a computer vision model’s poultry estimation accuracy by ~10% (~2,000 barns) using buffer analysis with Google Earth Engine, enhancing RAFI’s dashboard to capture market trends of 20,000+ poultry barns in North Carolina\\n\\nPresented a technical brief outlining data merging strategies, dashboard development, market capture and model enhancements to provide the client with a clear understanding of technical product and analysis results\\n\\n\\n\\n\\tIllinois department of employment safety\\tChicago, IL\\n\\n\\tState Employment Government Agency | Data analytics fellow at Policy Lab using R, Google Suites\\t Sept 2023 – Dec 2023\\n\\nSupported clients’ promotion of Unemployment Insurance equity by using SQL to extract datasets on 12,000 employees and 2,000+ applicants to create a merged individual level dataset with employment flux and application information\\n\\nDeveloped a metrics to identify the gap in laid-off workers’ participation in Unemployment Insurance and visualize it in Google Sheet and Slides to evaluate sectorial participation—region, industry, union vs. non-union, and company size\\n\\nCoordinated timely communications with the client on refining government outreach strategies and preventing further applicant attrition by maintaining information updates and delivering high-quality policy memo and presentation\\n\\n\\n\\n\\n\\nResearch publication\\n\\n\\tKuda-Singappulige, G., Green, E., Reichert, J., Bao Y., & Gruschow, K. (2025). Veil of darkness analysis of Illinois traffic stop data. Illinois Criminal Justice Information Authority.\\n\\nskills\\n\\n\\n\\nMS Office Suite: Word, Excel, Power Point, One Driver, Teams\\n\\nAI solutions: API integration, AWS cloud, prompt engineering, UI/UX design, data migration, deployment\\n\\nMachine Learning: feature engineering, dimensionality reduction, regularization, tree-based models, SVM, clustering, KNN\\n\\nData Visualization: Tableau, R(Shiny, sf, tmap, ggspatial, ggplot2), Python(Streamlit, GeoPandas, Seaborn, Matplotlib)\\n\\nNLP: Tokenization, summarization, embeddings, sentiment analysis, knowledge graphs, named entity recognition\\n\\nSQL, PostgreSQL: data management, filtering, aggregation, joining, extracting, interacting with R and Python\\n\\nPython (Boto, Langchain, Openai LLamaIndex, Pandas, NumPy, SpaCy, BeautifulSoup, Selenium)\\n\\nR (data.table, date.time, dplyr, tidyverse, plotly, viridis, Shiny)\\n\\nMS Office Suite: Word, Excel, Power Point, One Driver, Teams')]\n"
     ]
    }
   ],
   "source": [
    "loader = Docx2txtLoader(\"resume.docx\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m embeddings = HuggingFaceEmbeddings(model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Sample sentence or list of sentences\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# sentences = [\"This is a sample sentence.\"]\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Embed the first sentence\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Print the embeddings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# len(embedding)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/panda-hackathon/panda-hack/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:108\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    100\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    101\u001b[39m \n\u001b[32m    102\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m \u001b[33;03m        Embeddings for the text.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/panda-hackathon/panda-hack/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:79\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m texts = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multi_process:\n\u001b[32m     81\u001b[39m     pool = \u001b[38;5;28mself\u001b[39m._client.start_multi_process_pool()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/panda-hackathon/panda-hack/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:79\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m), texts))\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multi_process:\n\u001b[32m     81\u001b[39m     pool = \u001b[38;5;28mself\u001b[39m._client.start_multi_process_pool()\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(embeddings.embed_query('hello')))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['71e2a01d-238b-4be2-a020-1c2d01e635a3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to a list\n",
    "documents = data\n",
    "\n",
    "# Generate unique IDs for each document\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "# Add doc to vector store\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='71e2a01d-238b-4be2-a020-1c2d01e635a3', metadata={'source': 'resume.docx'}, page_content='YIJIN BAO\\n\\n\\t\\t+1-773-490-7200\\n\\n\\t\\tChicago, Illinois\\n\\n\\t\\tkay062@uchicago.edu, LinkedIn | GitHub\\n\\n\\n\\n\\n\\neducation\\n\\n\\tHarris School of Public Policy, University of Chicago\\tChicago, IL\\n\\n\\tData Analytics Specialization of Master of Public Policy (MPP), Graduate with Honor\\tJune 2024\\n\\nData Science: ETL pipeline, NLP, Deep Learning, Machine Learning, Computer Vision\\n\\nData Analytics: data collection, management, cleaning, manipulation, sanity check, analysis, visualization, GIS\\n\\nPublic Policy analysis: modeling and research for transportation policy, healthcare policy, urban policy\\n\\nBusiness data product: dashboard design, interactive chatbot, database management, automated workflows\\n\\n\\n\\n\\tShanghai International Studies University\\tShanghai, China\\n\\n\\tDual Bachelor of Laws and English, Outstanding Undergraduate Thesis Honor\\tJuly 2022\\n\\nInternational Law; International Economics Law; International Finance Law\\n\\nHonored Student Scholarship in 2018-2019 Academic Year\\n\\nStudent Union President (2020-2021): maintained alumni network and organize career networking events\\n\\n\\n\\nexperience\\n\\n\\tUnited nations statisticS DIVISION\\tHybrid\\n\\n\\tStatistical Intern using Python\\t February 2025 – June 2025\\n\\nDeveloped a web-scraping program using Amazon Bedrock, called Claude Sonnet 3 model and engineered prompt to optimize AI’s performance of the accurate collection of unstructured event data from international organizations’ websites\\n\\nAutomated and the scraping and processing of user interaction data of Global Network of Data Officers and Statisticians, continuously refining the process and analyzing participation trends to enhance outreach and network management\\n\\n\\n\\nIllinois Criminal Justice information authority\\t                                                                               Chicago, IL\\n\\n\\tState Government Agency | Research Analysis Intern using R, SQL\\tOct 2024 – Mar 2025, June 2023 – Aug 2023\\n\\nEngineered a highly automated data pipeline to efficiently extract from PDF, convert to R-readable format and integrate 50 years of federal disposition data, pre-processed data for analysis and conduct sanity check with complete documentation  \\n\\nCleaned, validated, and transformed a 20GB+ Illinois Traffic Stop dataset (2004–2022) to ensure data quality, standardize formats, and generate performance metrics, enabling statistical analysis of equity in police pull-overs, including visualizing hit rates and false alarm rates by race and police agency as part of the Veil of Darkness analysis\\n\\nAudited data of Illinois Death in Custody Reporting from 2019 to 2024 by developing a program to automate matching and identify unreported incidents through cross-referencing manually collected media-reported cases \\n\\n\\n\\n\\tRural Advancement Foundation International (RAFI)\\tChicago, IL\\n\\n\\tNGO for Economic Justice | Data science fellow at Data Science Clinic using Python\\t Jan 2024 – Mar 2024\\n\\nDeveloped and maintained a Streamlit dashboard to visualize 30 years of market trends across 3,455 auction houses, supporting RAFI’s efforts to convey findings to a Federal Government Agency for digitalization of contracts\\n\\nImproved a computer vision model’s poultry estimation accuracy by ~10% (~2,000 barns) using buffer analysis with Google Earth Engine, enhancing RAFI’s dashboard to capture market trends of 20,000+ poultry barns in North Carolina\\n\\nPresented a technical brief outlining data merging strategies, dashboard development, market capture and model enhancements to provide the client with a clear understanding of technical product and analysis results\\n\\n\\n\\n\\tIllinois department of employment safety\\tChicago, IL\\n\\n\\tState Employment Government Agency | Data analytics fellow at Policy Lab using R, Google Suites\\t Sept 2023 – Dec 2023\\n\\nSupported clients’ promotion of Unemployment Insurance equity by using SQL to extract datasets on 12,000 employees and 2,000+ applicants to create a merged individual level dataset with employment flux and application information\\n\\nDeveloped a metrics to identify the gap in laid-off workers’ participation in Unemployment Insurance and visualize it in Google Sheet and Slides to evaluate sectorial participation—region, industry, union vs. non-union, and company size\\n\\nCoordinated timely communications with the client on refining government outreach strategies and preventing further applicant attrition by maintaining information updates and delivering high-quality policy memo and presentation\\n\\n\\n\\n\\n\\nResearch publication\\n\\n\\tKuda-Singappulige, G., Green, E., Reichert, J., Bao Y., & Gruschow, K. (2025). Veil of darkness analysis of Illinois traffic stop data. Illinois Criminal Justice Information Authority.\\n\\nskills\\n\\n\\n\\nMS Office Suite: Word, Excel, Power Point, One Driver, Teams\\n\\nAI solutions: API integration, AWS cloud, prompt engineering, UI/UX design, data migration, deployment\\n\\nMachine Learning: feature engineering, dimensionality reduction, regularization, tree-based models, SVM, clustering, KNN\\n\\nData Visualization: Tableau, R(Shiny, sf, tmap, ggspatial, ggplot2), Python(Streamlit, GeoPandas, Seaborn, Matplotlib)\\n\\nNLP: Tokenization, summarization, embeddings, sentiment analysis, knowledge graphs, named entity recognition\\n\\nSQL, PostgreSQL: data management, filtering, aggregation, joining, extracting, interacting with R and Python\\n\\nPython (Boto, Langchain, Openai LLamaIndex, Pandas, NumPy, SpaCy, BeautifulSoup, Selenium)\\n\\nR (data.table, date.time, dplyr, tidyverse, plotly, viridis, Shiny)\\n\\nMS Office Suite: Word, Excel, Power Point, One Driver, Teams')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "retriever.invoke(\"Looking for data engineering job\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda-hack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
